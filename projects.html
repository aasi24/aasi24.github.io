<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Work Experience - Ahmad Ahsan Saleem">
    <title>Projects</title>
	<link rel="icon" href="images/ml.ico" type="image/x-icon">
    <link rel="stylesheet" href="css/style.css"> <!-- Link to external CSS -->
</head>
<body>

    <!-- Header Navigation -->
    <header>
        <h1>Ahmad Ahsan Saleem</h1>
        <nav>
			<a href="index.html">Home</a>
            <a href="education.html">Education</a>
            <a href="work_experience.html">Work Experience</a>
            <a href="projects.html">Projects</a>
			<a href="skills.html">Skills</a>
			<a href="certifications.html">Certifications</a>
            <a href="contact.html">Contact</a>
        </nav>
    </header>
	
	 <!-- Projects Section -->
    <section class="projects-section">
        <!-- Section 1: Industrial IoT (IIoT) -->
        <div class="project-section-main">
            <h2>Industrial IoT (IIoT) and Manufacturing Optimization</h2>
            <div class="project-grid">
				<div class="project-card" onclick="openModal('iiot-project0')">
                    <img src="images/IA.png"  alt="Auxiliaries Anomaly Detection">
                    <h3>Industrial Analytics</h3>
                    <p>Cloud-based industrial analytics for real-time insights.</p>
                </div>
                <div class="project-card" onclick="openModal('iiot-project1')">
                    <img src="images/ALM.png"  alt="Auxiliaries Anomaly Detection">
                    <h3>Auxiliaries Anomaly Detection</h3>
                    <p>Detecting anomalies in power plant auxiliary systems.</p>
                </div>
                <div class="project-card" onclick="openModal('iiot-project2')">
                    <img src="images/CTO.png" alt="Cooling Tower Optimization">
                    <h3>Cooling Tower Optimization</h3>
                    <p>Optimizing Cooling Towers to Reduce Energy Costs.</p>
                </div>
                <div class="project-card" onclick="openModal('iiot-project3')">
                    <img src="images/RO.png" alt="Reverse Osmosis Predictive Modelling">
                    <h3>Reverse Osmosis</h3>
                    <p>Predictive maintenance for water treatment systems.</p>
                </div>
                <div class="project-card" onclick="openModal('iiot-project4')">
                    <img src="images/SolarIntensityForecating.png" alt="SolarIntensityForecasting">
                    <h3>Solar Intensity Forecasting</h3>
                    <p>Considers sun energy, physics, economics variability.</p>
                </div>
				<div class="project-card" onclick="openModal('iiot-project5')">
                    <img src="images/CSF.png" alt="SolarIntensityForecasting">
                    <h3>Corrosion Scaling Fouling (CSF)</h3>
                    <p>Predictive models for scaling and corrosion prevention.</p>
                </div>
            </div>
        </div>

        <!-- Section 2: E-commerce and Customer Analytics -->
        <div class="project-section">
            <h2>E-commerce and Customer Analytics</h2>
            <div class="project-grid">
                <div class="project-card" onclick="openModal('ecommerce-project1')">
                    <img src="images/E-commerce.webp" alt="E-commerce Customer Behavior">
                    <h3>E-commerce Customer Behavior</h3>
                    <p>Customer segmentation and satisfaction drivers.</p>
                </div>
                <div class="project-card" onclick="openModal('ecommerce-project2')">
                    <img src="images/CustomerChurnPrediction.webp" alt="Customer Churn Prediction">
                    <h3>Customer Churn Prediction</h3>
                    <p>Churn prediction models for banking customers.</p>
                </div>
                <div class="project-card" onclick="openModal('ecommerce-project3')">
                    <img src="images/Insurance Premium Prediction.webp" alt="Insurance Premium Prediction">
                    <h3>Insurance Premium Prediction</h3>
                    <p>Predicting insurance premiums based on customer data.</p>
                </div>
            </div>
        </div>

        <!-- Section 3: Finance and Business Analytics -->
        <div class="project-section">
            <h2>Finance and Geospatial Business Analytics</h2>
            <div class="project-grid">
                <div class="project-card" onclick="openModal('finance-project1')">
                    <img src="images/Banking Churn Prediction.webp" alt="Banking Churn Prediction">
                    <h3>Banking Churn Prediction</h3>
                    <p>Data-driven insights for bank customer behavior.</p>
                </div>
                <div class="project-card" onclick="openModal('finance-project2')">
                    <img src="images/gas.webp" alt="City Businesses Analysis">
                    <h3>City Businesses Analysis</h3>
                    <p>Analyzing business proximity patterns in urban areas.</p>
                </div>
                <div class="project-card" onclick="openModal('finance-project3')">
                    <img src="images/auto.webp" alt="Auto Parts Business Analysis">
                    <h3>Auto Parts Business Analysis</h3>
                    <p>Extracting market insights for targeted auto parts promotions.</p>
                </div>
				<div class="project-card" onclick="openModal('finance-project4')">
                    <img src="images/autobusiness.png" alt="Auto Parts Management System">
                    <h3>Auto Parts Management System</h3>
                    <p>Auto parts management with interactive analytics..</p>
                </div>
            </div>
        </div>

        <!-- Section 4: Healthcare and Risk Prediction -->
        <div class="project-section">
            <h2>Healthcare Prediction and Large Language Model</h2>
            <div class="project-grid">
                <div class="project-card" onclick="openModal('healthcare-project1')">
                    <img src="images/Healthcare Classification.webp" alt="Medical Insurance Cost Prediction">
                    <h3>Medical Insurance Cost Prediction</h3>
                    <p>Predicting healthcare charges based on factors like smoking and age.</p>
                </div>
                <div class="project-card" onclick="openModal('healthcare-project2')">
                    <img src="images/shellfish.webp" alt="Abalone Case Study">
                    <h3>Abalone Case Study</h3>
                    <p>Predicting abalone age using clustering techniques.</p>
                </div>
                <div class="project-card" onclick="openModal('healthcare-project3')">
                    <img src="images/symmetry.webp" alt="MLP for Classification">
                    <h3>Project Symmetry</h3>
                    <p>Semantic Comparison Tool.</p>
                </div>
            </div>
        </div>
		
		<!-- Section 4: Softwares and Apps -->
        <div class="project-section">
            <h2>Softwares and Apps</h2>
            <div class="project-grid">
                <div class="project-card" onclick="openModal('soft-project1')">
                    <img src="images/tech.webp" alt="The Technician App">
                    <h3>The Technician App</h3>
                    <p>Automotive Maintenance Management System.</p>
                </div>
				<div class="project-card" onclick="openModal('soft-project2')">
                    <img src="images/Driver.webp" alt="Driver App">
                    <h3>Driver App</h3>
                    <p>Task Management and Communication System.</p>
                </div>
				<div class="project-card" onclick="openModal('soft-project3')">
                    <img src="images/barcode.png" alt="BarCode App">
                    <h3>BarCode App</h3>
                    <p>Inventory Management and Barcode Generation.</p>
                </div>
                
            </div>
	
        </div>
		
    </section>

    <!-- Modals for Project Details -->

    <!-- Modal for Industrial IoT Projects -->
    <!-- Modal for Auxiliaries Load Manager (ALM) -->
	<div id="iiot-project1" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('iiot-project1')">&times;</span>

				<h2>Project Title: Auxiliaries Load Model (ALM)</h2>
				<p><strong>Company:</strong> Engro Digital</p>
				<p><strong>Client:</strong> Engro Powergen Qadirpur Limited (EPQL)</p>

				<h3>Scope of the Project:</h3>
				<p>To develop a data-driven solution aimed at identifying anomalies in the auxiliary load of power plants and enhancing operational efficiency. This project involved creating predictive models for auxiliary load management to detect abnormal equipment performance and optimize plant processes.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li>Developed machine learning models to predict the expected auxiliary load and detect deviations.</li>
					<li>Implemented monitoring for key auxiliary equipment, including condensate pumps, vacuum pumps, cooling tower fans, and pumps.</li>
					<li>Performed exploratory data analysis (EDA) and inferential data analysis (IDA) to identify patterns and anomalies in equipment performance.</li>
					<li>Integrated real-time operational data from DCS historian, LIMS, and plant data sources for model training.</li>
					<li>Created visualization tools for operators to review historical trends and identify equipment-level anomalies.</li>
					<li>Designed models to correlate process parameters like gross load, temperature, and pressure with auxiliary load behavior.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Machine Learning:</strong> Random Forest Regressor, Linear Regression, SGD Regressor</li>
					<li><strong>Data Science:</strong> Python, scikit-learn, K-fold Cross Validation</li>
					<li><strong>Database and ETL:</strong> SQL, cloud data integration</li>
					<li><strong>Visualization Tools:</strong> Matplotlib, seaborn for anomaly detection visualizations</li>
					<li><strong>Data Preparation:</strong> Robust scaling, outlier removal, normalization, feature engineering</li>
					<li><strong>Cloud Infrastructure:</strong> AWS for deployment and real-time data processing</li>
				</ul>

				<h3>Timeline:</h3>
				<p>Project initiated in Q1 2018, completed Q2 2019. Continuous refinement and optimization through testing phases with 20% of data reserved for validation.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Data Collection:</strong> Gathered and preprocessed over 610,000 records from plant operations covering key process parameters such as gross load, ambient temperature, and specific equipment data like amperes from condensate pumps, vacuum pumps, and cooling tower fans.</li>
					<li><strong>Model Development:</strong> Implemented several models including:
						<ul>
							<li>Total plant auxiliary load model.</li>
							<li>Equipment-specific models for HP/LP pumps, condensate pumps, and cooling tower fans.</li>
							<li>Regression models to detect abnormal equipment behavior based on real-time data.</li>
						</ul>
					</li>
					<li><strong>Anomaly Detection:</strong> Anomalies were identified by comparing actual equipment performance to expected load behavior. If deviations were found, root cause analysis was initiated to determine if the abnormality was caused by process changes or equipment faults.</li>
					<li><strong>Integration with Plant Systems:</strong> Real-time monitoring was established, enabling plant engineers to make informed decisions based on auxiliary load predictions.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Operational Efficiency:</strong> ALM was deployed in Engro Powergen Qadirpur's 217MW power plant, which utilizes permeate gas for reduced emissions.</li>
					<li><strong>Fault Detection:</strong> The system allowed plant operators to quickly identify inefficiencies and abnormal operations in auxiliaries, contributing to energy savings and reduced wear on equipment.</li>
					<li><strong>Real-time Monitoring:</strong> By offering insights into historical trends and real-time data, the ALM system helped engineers optimize auxiliary equipment performance, prevent downtime, and improve overall plant reliability.</li>
				</ul>
			</div>
		</div>
		
		<!-- Modal for Cooling Tower Optimization (CTO) -->
		<div id="iiot-project2" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('iiot-project2')">&times;</span>

				<h2>Project Title: Cooling Tower Optimization (CTO)</h2>
				<p><strong>Company:</strong> Engro Digital</p>
				<p><strong>Client:</strong> Engro Powergen Qadirpur Limited (EPQL)</p>

				<h3>Scope of the Project:</h3>
				<p>To optimize the operation of cooling tower fans and pumps in a power plant to reduce energy consumption and operational costs. The aim was to balance plant thermodynamics with minimal energy usage while maintaining safe operating conditions.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li>Developed predictive models to optimize the operation of cooling tower fans and pumps.</li>
					<li>Collected real-time data related to cooling tower performance, including fan speeds, pump flows, and ambient conditions.</li>
					<li>Conducted energy optimization to ensure cooling requirements were met with minimal fan and pump operation.</li>
					<li>Performed exploratory data analysis (EDA) to understand the relationship between cooling tower operations and overall plant efficiency.</li>
					<li>Created recommendations for adjusting fan speeds and pump usage in response to changes in load, weather conditions, and process requirements.</li>
					<li>Collaborated with plant engineers to fine-tune the system for real-time control.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Data Science:</strong> Python, scikit-learn for predictive modeling and regression analysis.</li>
					<li><strong>Machine Learning Models:</strong> Linear Regression, Random Forest, and time-series models for energy optimization.</li>
					<li><strong>Data Collection:</strong> Integrated with plant’s SCADA system for real-time data on pump flows, fan speeds, and temperature conditions.</li>
					<li><strong>Cloud Infrastructure:</strong> AWS for data storage, processing, and model deployment.</li>
				</ul>

				<h3>Timeline:</h3>
				<p>The project began in Q2 2020 and completed in Q4 2020, with ongoing refinements and improvements through real-time monitoring and testing.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Data Collection:</strong> Collected data from the plant’s cooling tower equipment, including pump flow rates, fan operation speeds, ambient temperature, and water supply temperatures. Analyzed over 1.2 million records from various sensors and operational systems to identify patterns and inefficiencies.</li>
					<li><strong>Model Development:</strong> Developed machine learning models to predict optimal fan and pump usage under varying conditions (weather, load, temperature). Created algorithms to adjust fan speeds and pump flows dynamically to ensure minimal energy usage while maintaining required cooling performance.</li>
					<li><strong>Energy Optimization:</strong> The models focused on reducing the number of running fans and pumps when possible, based on real-time ambient conditions and heat load. The system optimized the cooling tower’s performance by adjusting the combination of running fans and pumps to meet the plant’s cooling needs with minimal power consumption.</li>
					<li><strong>Integration with Plant Systems:</strong> Integrated with the plant’s control system for real-time feedback and control, enabling automatic adjustments to fan speeds and pump operation.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Energy Savings:</strong> The system significantly reduced energy consumption by optimizing the operation of fans and pumps, leading to cost savings in power usage.</li>
					<li><strong>Improved Plant Efficiency:</strong> By maintaining optimal cooling at reduced energy costs, the project contributed to overall plant efficiency.</li>
					<li><strong>Real-time Monitoring:</strong> The system provided real-time insights into fan and pump performance, enabling operators to make adjustments and avoid unnecessary energy use.</li>
					<li><strong>Sustainability:</strong> The project supported the plant’s goal of reducing its carbon footprint by minimizing energy usage for auxiliary equipment.</li>
				</ul>

			</div>
		</div>


			<!-- Modal for Reverse Osmosis Predictive Modelling -->
		<div id="iiot-project3" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('iiot-project3')">&times;</span>

				<h2>Project Title: Reverse Osmosis Predictive Modelling</h2>
				<p><strong>Company:</strong> Engro Digital</p>
				<p><strong>Client:</strong> First National Operation & Maintenance Co. Ltd</p>

				<h3>Scope of the Project:</h3>
				<p>The goal of this project was to optimize the Reverse Osmosis (RO) process in a desalination plant by developing predictive models to forecast membrane fouling and scaling, which could degrade system performance. The aim was to ensure high water quality and reduce maintenance costs by predicting potential operational inefficiencies in real-time.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li>Developed predictive models using machine learning to detect and forecast membrane fouling and scaling.</li>
					<li>Integrated real-time data from sensors across the desalination plant, including flow rates, pressure differentials, water quality metrics, and membrane performance indicators.</li>
					<li>Conducted data preprocessing to clean sensor data, removing outliers and imputing missing values.</li>
					<li>Designed dashboards for plant operators to visualize key RO parameters and identify early signs of system inefficiencies.</li>
					<li>Implemented anomaly detection algorithms for proactive maintenance scheduling to avoid unplanned downtimes.</li>
					<li>Created a feedback loop to update the models based on the latest operational data for continuous improvement.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Machine Learning Models:</strong> Random Forest, Linear Regression, Gradient Boosting for predictive modelling.</li>
					<li><strong>Data Science:</strong> Python, scikit-learn, pandas, and NumPy for data manipulation and analysis.</li>
					<li><strong>Cloud Infrastructure:</strong> AWS for deployment and real-time data processing.</li>
					<li><strong>Data Visualization:</strong> Power BI and Matplotlib for generating insightful graphs and charts.</li>
					<li><strong>Data Sources:</strong> SCADA, Plant Information Systems, and sensor networks for flow, pressure, and water quality metrics.</li>
					<li><strong>Database:</strong> SQL for data storage and management.</li>
				</ul>

				<h3>Timeline:</h3>
				<p>The project commenced in Q1 2021 and was successfully completed in Q4 2021 with ongoing updates and performance reviews.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Data Collection and Preprocessing:</strong> Gathered over 1 million data points from the RO plant sensors, including membrane pressures, flow rates, feed water salinity, and chemical dosing levels. Cleaned and transformed the data using scaling and normalization techniques to prepare it for model training.</li>
					<li><strong>Model Development:</strong> Developed multiple models for membrane fouling prediction, identifying early-stage fouling based on pressure differentials and permeate flow. Incorporated external factors such as feed water quality (TDS, salinity) and temperature to enhance model accuracy.</li>
					<li><strong>Predictive Maintenance:</strong> Anomaly detection models were integrated to alert operators about potential failures, reducing unscheduled maintenance by 15%. The predictive models were deployed on the cloud to run in real-time, providing continuous insights into the system's performance.</li>
					<li><strong>Optimization and Feedback Loop:</strong> A feedback loop was established where the models were retrained periodically based on new data from the plant, ensuring the system remained adaptive to changing operational conditions.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Enhanced Water Quality:</strong> Predictive models enabled plant operators to maintain optimal membrane performance, ensuring consistent water quality.</li>
					<li><strong>Cost Reduction:</strong> The system reduced unplanned maintenance and chemical usage, resulting in an estimated 20% reduction in operational costs.</li>
					<li><strong>Increased Operational Uptime:</strong> The proactive maintenance recommendations minimized unexpected downtimes, improving overall plant availability and performance.</li>
					<li><strong>Real-time Monitoring:</strong> The dashboard provided real-time alerts, allowing the plant team to make quick decisions and avoid potential process inefficiencies.</li>
				</ul>

			</div>
		</div>


    <!-- Modal for Solar Intensity Forecasting - DNI -->
	<div id="iiot-project4" class="modal">
		<div class="modal-content">
			<span class="close" onclick="closeModal('iiot-project4')">&times;</span>

			<h2>Project Title: Solar Intensity Forecasting - DNI</h2>
			<p><strong>Company:</strong> Engro Digital</p>
			<p><strong>Client:</strong> Morocco (Internal Research Project)</p>

			<h3>Scope of the Project:</h3>
			<p>To develop a solar intensity forecasting model using historical solar radiation data and seasonal patterns to predict future Direct Normal Irradiance (DNI) for optimal solar power generation. The project aimed to achieve accurate, year-long weather and solar intensity predictions using advanced machine learning techniques.</p>

			<h3>Key Responsibilities:</h3>
			<ul>
				<li>Developed a machine learning-based forecasting model to predict solar intensity using historical sensor data.</li>
				<li>Conducted ETL (Extract, Transform, Load) operations to gather data from CSV files, databases, and other sources.</li>
				<li>Cleaned the data by removing junk values and interpolating missing data points for accurate modeling.</li>
				<li>Performed Exploratory Data Analysis (EDA) and Inferential Data Analysis (IDA) to identify trends, patterns, and anomalies.</li>
				<li>Built time-series forecasting models, including using Facebook Prophet, to predict solar intensity based on weather and seasonal factors.</li>
				<li>Deployed the forecasting solution on AWS SageMaker and used AWS S3 for real-time data storage and processing.</li>
				<li>Created visualizations to present solar intensity trends and forecast accuracy to stakeholders.</li>
			</ul>

			<h3>Technologies and Tools Used:</h3>
			<ul>
				<li><strong>Machine Learning Models:</strong> Time-series forecasting models (Facebook Prophet).</li>
				<li><strong>Data Science Tools:</strong> Python, scikit-learn, pandas, NumPy for data cleaning, manipulation, and analysis.</li>
				<li><strong>Cloud Infrastructure:</strong> AWS SageMaker for model deployment and AWS S3 for data storage.</li>
				<li><strong>Data Engineering:</strong> SQL and PostgreSQL for data handling, Flask for model deployment, and Power BI for visualization.</li>
				<li><strong>Visualization Tools:</strong> Matplotlib, Seaborn for creating trend and pattern analysis graphs.</li>
			</ul>

			<h3>Timeline:</h3>
			<p>Project started in Q3 2020 and completed in Q1 2021, with continuous testing and refinement based on forecast performance.</p>

			<h3>Solution Design and Implementation:</h3>
			<ul>
				<li><strong>Data Collection and Preprocessing:</strong> Integrated data from multiple sources such as weather sensors, CSV files, and external databases, gathering data on solar intensity, temperature, humidity, and other seasonal factors. Preprocessed the data by removing outliers, handling missing values through interpolation, and normalizing for model training.</li>
				<li><strong>Model Development:</strong> Time-series forecasting models were developed to predict solar intensity (DNI) for the next year using Facebook Prophet. The model was designed to account for seasonal variations, weather patterns, and historical data trends to achieve accurate predictions.</li>
				<li><strong>Visualization and Reporting:</strong> Solar intensity forecasts were visualized in Power BI to provide stakeholders with clear insights into expected performance. The model achieved a forecasting accuracy of 92%, enabling better planning for solar power generation.</li>
				<li><strong>Deployment:</strong> The final model was deployed using AWS SageMaker for real-time forecasting, and AWS S3 was used to store both historical data and model predictions.</li>
			</ul>

			<h3>Deployment and Impact:</h3>
			<ul>
				<li><strong>High Accuracy Forecasts:</strong> Achieved 92% accuracy in solar intensity predictions, significantly improving the reliability of solar energy production planning.</li>
				<li><strong>Improved Resource Planning:</strong> Enabled the client to better plan and optimize solar energy production based on expected solar intensity.</li>
				<li><strong>Efficient Cloud Deployment:</strong> The use of AWS SageMaker and S3 allowed for scalable, real-time data processing and model updates.</li>
			</ul>

		</div>
	</div>
	
	<!-- Modal for Corrosion Scaling Fouling (CSF) -->
	<div id="iiot-project5" class="modal">
		<div class="modal-content">
			<span class="close" onclick="closeModal('iiot-project5')">&times;</span>

			<h2>Project Title: Corrosion Scaling Fouling (CSF)</h2>
			<p><strong>Company:</strong> Engro Digital</p>
			<p><strong>Client:</strong> Engro Fertilizers Ltd.</p>

			<h3>Scope of the Project:</h3>
			<p>To develop a simulation model that predicts and optimizes chemical consumption in cooling water systems to control corrosion, scaling, and fouling. The system utilizes real-time plant data and machine learning models to optimize chemical dosing, improving heat exchanger performance and reducing operational costs.</p>

			<h3>Key Responsibilities:</h3>
			<ul>
				<li>Developed machine learning models to predict scaling, corrosion, and fouling potential in cooling systems.</li>
				<li>Collected real-time data from the plant, including water chemistry, temperature, and flow rates, from DCS historian, EZE monitor, and LIMS.</li>
				<li>Implemented models to simulate the cooling water system's chemical dosing for anti-scalants, corrosion inhibitors, and bio-dispersants.</li>
				<li>Performed data analysis on historical operational data to understand patterns and key parameters affecting corrosion and scaling.</li>
				<li>Developed predictive models to recommend optimal operating conditions for heat exchangers, maintaining safe equipment limits and enhancing performance.</li>
				<li>Visualized key performance metrics such as pH, calcium hardness, and total dissolved solids to assist plant operators in monitoring system health.</li>
			</ul>

			<h3>Technologies and Tools Used:</h3>
			<ul>
				<li><strong>Machine Learning:</strong> Python, Random Forest, Linear Regression.</li>
				<li><strong>Cloud Infrastructure:</strong> GE Predix platform for real-time data processing and model deployment.</li>
				<li><strong>Data Science Tools:</strong> scikit-learn, NumPy, and pandas for data analysis.</li>
				<li><strong>Data Integration:</strong> SQL for database management, and DCS historian for collecting operational data.</li>
				<li><strong>Visualization Tools:</strong> Power BI, Matplotlib for trend analysis and monitoring of key parameters.</li>
			</ul>

			<h3>Timeline:</h3>
			<p>The project was initiated in Q2 2019 and completed in Q1 2020, with continuous optimization through real-time model feedback and data refinement.</p>

			<h3>Solution Design and Implementation:</h3>
			<ul>
				<li><strong>Data Collection and Preprocessing:</strong> Integrated data from plant systems like the DCS historian, LIMS, and real-time sensors to capture operational parameters such as water temperature, calcium hardness, and chemical dosing. Preprocessed data by cleaning, removing outliers, and normalizing to ensure accurate model predictions.</li>
				<li><strong>Model Development:</strong> Developed machine learning models for predicting corrosion and scaling tendencies using real-time operating data. Integrated the model with the plant’s existing infrastructure, ensuring real-time chemical dosing adjustments based on predictions.</li>
				<li><strong>Optimization and Prediction:</strong> The models dynamically predicted optimal chemical dosing for anti-scalants, corrosion inhibitors, and bio-dispersants based on water chemistry parameters. Continuous real-time monitoring allowed for adjustments to maintain the heat exchanger’s performance and avoid deposition.</li>
				<li><strong>Visualization and Reporting:</strong> The model provided a real-time dashboard, showing key metrics such as pH, calcium levels, and corrosion rate, enabling operators to make data-driven decisions.</li>
			</ul>

			<h3>Deployment and Impact:</h3>
			<ul>
				<li><strong>Operational Efficiency:</strong> The CSF system improved the efficiency of heat exchangers by preventing scale and corrosion, reducing downtime and maintenance costs.</li>
				<li><strong>Cost Savings:</strong> By optimizing chemical dosing, the project achieved a significant reduction in chemical usage, contributing to overall cost savings for the plant.</li>
				<li><strong>Real-time Monitoring:</strong> The deployment allowed for continuous real-time monitoring and feedback, improving the plant’s ability to maintain operational limits and optimize performance.</li>
			</ul>

		</div>
	</div>
	
	<!-- Modal for IA Data Analysis and Prediction Services -->
	<div id="iiot-project0" class="modal">
		<div class="modal-content">
			<span class="close" onclick="closeModal('iiot-project0')">&times;</span>

			<h2>Project Title: IA Data Analysis and Prediction Services</h2>
			<p><strong>Company:</strong> EmpiciAI</p>

			<h3>Scope of the Project:</h3>
			<p>The project aimed to develop and deploy a robust, scalable suite of services for data analysis and prediction in industrial environments. These services included preprocessing, complex unit conversions, trend analysis, correlation analysis, chemical property calculations, and prediction modeling. The solution was designed to handle large volumes of real-time industrial data efficiently using AWS cloud services, Kafka, and Spark for data storage and processing. Key Performance Indicators (KPIs) were established to ensure the accuracy, performance, and scalability of the system.</p>

			<h3>Key Responsibilities:</h3>
			<ul>
				<li>Led the development of data preprocessing tools, including filters for spike & dip detection, flatline identification, and smoothing algorithms.</li>
				<li>Designed and implemented unit conversion services to normalize data from multiple sources and measurement systems.</li>
				<li>Developed APIs for statistical analysis, correlation computation, and chemical property calculations.</li>
				<li>Built predictive models using FBProphet for trend forecasting and anomaly detection based on historical data patterns.</li>
				<li>Integrated AWS services like Cognito for user authentication, Lambda for serverless compute, and S3 for storing processed data.</li>
				<li>Deployed Kafka and Spark for real-time data ingestion, processing, and storage, ensuring low-latency data handling.</li>
				<li>Utilized AWS Glue for data extraction, transformation, and loading (ETL) processes to aggregate and process large datasets.</li>
				<li>Managed OPC UA connectors to integrate real-time industrial data from various sources into the system.</li>
			</ul>

			<h3>Technologies and Tools Used:</h3>
			<ul>
				<li><strong>Languages:</strong> Python, Java</li>
				<li><strong>Frameworks:</strong> Spring Boot (for Java services), Flask (for Python services)</li>
				<li><strong>APIs & Tools:</strong> FBProphet for time series forecasting, OPC UA for real-time data ingestion, Redis for in-memory caching, and statistical libraries like NumPy and SciPy for data analysis</li>
				<li><strong>Databases:</strong> RDS-PostgreSQL for structured data storage, Spark for distributed data processing, and Kafka for real-time data streams</li>
				<li><strong>AWS Services:</strong> Cognito (user authentication), Lambda (serverless compute), S3 (data storage), Glue (ETL), GreenGrass (edge computing)</li>
				<li><strong>Version Control:</strong> Git</li>
				<li><strong>Others:</strong> Docker (for containerized deployments), Kafka (for real-time data streaming and ingestion), Redis (caching to improve performance)</li>
			</ul>

			<h3>Timeline:</h3>
			<p>September 2020 – March 2022</p>

			<h3>Solution Design and Implementation:</h3>
			<ul>
				<li><strong>Preprocessing Tools:</strong> Implemented data filters like Spike & Dip, flatline, and smoothing filters to ensure clean, accurate data for further analysis.</li>
				<li><strong>Complex Unit Conversions:</strong> Built services that performed automatic unit conversions across different data formats, ensuring data normalization across global datasets.</li>
				<li><strong>Statistical and Correlation Analysis:</strong> Developed APIs for performing advanced statistical tests (e.g., Z-tests, T-tests) and generating correlation matrices to provide real-time insights into data relationships.</li>
				<li><strong>Prediction Models:</strong> Integrated FBProphet for accurate forecasting of trends and anomalies based on historical patterns.</li>
				<li><strong>Chemical Property Calculations:</strong> Designed APIs to calculate chemical properties (e.g., gas density, Wobbe index) based on real-time data from industrial processes.</li>
				<li><strong>Trend Analysis:</strong> Enabled trend detection by identifying patterns like spikes, dips, and flatlines in real-time, allowing clients to make data-driven decisions.</li>
				<li><strong>Data Ingestion and Processing:</strong> Integrated Kafka for real-time data streaming, ensuring low-latency data processing. Spark was used for distributed data processing to handle large-scale computations efficiently. AWS Glue was utilized for ETL processes to transform and clean data before storage.</li>
			</ul>

			<h3>KPIs Implemented:</h3>
			<ul>
				<li><strong>Data Accuracy Rate:</strong> Achieved a 95% improvement in data accuracy through automated anomaly detection and correction, such as spike & dip and flatline filters.</li>
				<li><strong>Prediction Accuracy:</strong> Maintained 90% prediction accuracy in forecasting models using FBProphet, significantly improving trend prediction and anomaly detection.</li>
				<li><strong>System Uptime:</strong> Leveraged AWS infrastructure and Docker for high availability, ensuring 99.9% uptime across the deployed services.</li>
				<li><strong>Data Throughput:</strong> The system handled over 1,000 data points per second using Kafka for real-time data streams and Spark for processing large datasets.</li>
				<li><strong>API Response Time:</strong> Optimized API response times to under 200 milliseconds by implementing caching strategies using Redis and tuning AWS Lambda for serverless compute efficiency.</li>
				<li><strong>Unit Conversion Accuracy:</strong> Implemented automated unit conversion with a 99% accuracy rate, ensuring seamless compatibility across diverse datasets.</li>
			</ul>

			<h3>AWS Services and Their Use:</h3>
			<ul>
				<li><strong>AWS Cognito:</strong> Managed secure authentication and user identity for the application, enabling fine-grained access control.</li>
				<li><strong>AWS Lambda:</strong> Enabled serverless computing for efficient, on-demand execution of critical services, reducing infrastructure costs and improving scalability.</li>
				<li><strong>AWS S3:</strong> Stored large volumes of raw and processed data, ensuring durability and scalability for the growing dataset.</li>
				<li><strong>AWS Glue:</strong> Automated the ETL process for ingesting, transforming, and storing large datasets, allowing seamless integration with other AWS services.</li>
				<li><strong>AWS GreenGrass:</strong> Enabled edge computing capabilities, processing data closer to industrial devices for real-time analysis and decision-making.</li>
				<li><strong>RDS-PostgreSQL:</strong> Hosted structured data, such as processed tags, metadata, and user information, offering robust relational data handling.</li>
				<li><strong>Apache Spark:</strong> Used for distributed data processing and analytics, allowing efficient handling of large-scale industrial datasets.</li>
			</ul>

			<h3>Deployment and Impact:</h3>
			<ul>
				<li><strong>Real-time Data Analysis:</strong> Improved predictive maintenance capabilities through real-time trend detection (spikes, dips, flatlines) and anomaly detection, reducing operational downtime for clients.</li>
				<li><strong>Chemical Properties:</strong> Delivered accurate chemical property calculations (e.g., gas density, Wobbe index) in real-time, providing actionable insights for industries dealing with complex chemical processes.</li>
				<li><strong>Scalability and Efficiency:</strong> Using Kafka and Spark, the system processed and stored thousands of data points per second, ensuring high data throughput and performance.</li>
				<li><strong>Cost Efficiency:</strong> By leveraging AWS Lambda and serverless architecture, the system reduced infrastructure costs while scaling efficiently to handle real-time industrial data.</li>
			</ul>

		</div>
	</div>
	
	
	<!-- Modal for E-commerce Customer Behavior Analysis -->
	<div id="ecommerce-project1" class="modal">
		<div class="modal-content">
			<span class="close" onclick="closeModal('ecommerce-project1')">&times;</span>

			<h2>Project Title: E-commerce Customer Behavior Analysis</h2>
			<p><strong>Company:</strong> University Canada West (Research-based)</p>

			<h3>Scope of the Project:</h3>
			<p>To analyze and predict customer behavior on an e-commerce platform using machine learning techniques. The project focused on customer segmentation, satisfaction prediction, and retention analysis based on factors like demographics, spending patterns, membership type, and satisfaction levels. This analysis aimed to improve targeted marketing strategies and enhance the overall customer experience.</p>

			<h3>Key Responsibilities:</h3>
			<ul>
				<li>Conducted Exploratory Data Analysis (EDA) on a dataset with 350 observations and 10 features, including age, total spend, items purchased, and satisfaction level.</li>
				<li>Developed a customer segmentation model using the K-Means clustering algorithm to identify distinct customer groups based on behavior and demographics.</li>
				<li>Built a classification model using K-Nearest Neighbors (KNN) to predict customer satisfaction levels, optimizing hyperparameters through GridSearchCV.</li>
				<li>Implemented a customer retention model using a Multi-Layer Perceptron (MLP) neural network to understand the impact of various factors on customer loyalty.</li>
				<li>Visualized key insights, including city-based behaviors, membership influence on spending, and purchase patterns to guide personalized marketing strategies.</li>
			</ul>

			<h3>Technologies and Tools Used:</h3>
			<ul>
				<li><strong>Machine Learning Algorithms:</strong> K-Means Clustering, K-Nearest Neighbors (KNN), Multi-Layer Perceptron (MLP).</li>
				<li><strong>Data Science Tools:</strong> Python, scikit-learn, pandas, NumPy for data manipulation and model building.</li>
				<li><strong>Data Visualization:</strong> Matplotlib, Seaborn for plotting distributions, purchase patterns, and cluster analysis.</li>
				<li><strong>Data Preprocessing:</strong> StandardScaler for normalization, OneHotEncoder for categorical data encoding, and LabelEncoder for target variable transformation.</li>
				<li><strong>Evaluation Metrics:</strong> Silhouette Score for clustering, Accuracy, Precision, and Recall for classification models.</li>
			</ul>

			<h3>Timeline:</h3>
			<p>The project was completed as part of the Machine Learning course at University Canada West, with key milestones achieved between January 2024 and March 2024.</p>

			<h3>Solution Design and Implementation:</h3>
			<ul>
				<li><strong>Data Analysis and Segmentation:</strong> Performed EDA to understand the distribution of age, spending, and purchase patterns. Implemented K-Means clustering with the Elbow Method to determine the optimal number of clusters, achieving an average silhouette score of 0.77. Identified six distinct customer segments, ranging from high-spending, loyal customers to budget-conscious, less engaged shoppers.</li>
				<li><strong>Customer Satisfaction Prediction:</strong> Employed KNN for classification to predict customer satisfaction based on demographic and behavioral features. Utilized GridSearchCV to fine-tune hyperparameters, resulting in a model that achieved 100% accuracy on the given dataset.</li>
				<li><strong>Customer Retention Analysis:</strong> Developed a customer retention model using a Multi-Layer Perceptron (MLP) neural network. Preprocessed data with scaling, encoding, and dropout layers to avoid overfitting. Achieved a model accuracy of 91.3%, indicating strong predictive power in identifying factors influencing customer retention.</li>
				<li><strong>Insights and Visualization:</strong> Provided city-based insights showing how regional variations affect customer behavior. Offered recommendations for personalized marketing strategies, loyalty programs, and discount-driven campaigns based on the analysis.</li>
			</ul>

			<h3>Deployment and Impact:</h3>
			<ul>
				<li><strong>Enhanced Customer Understanding:</strong> The models provided actionable insights into customer segmentation, satisfaction drivers, and retention mechanisms.</li>
				<li><strong>Targeted Marketing Strategies:</strong> The segmentation and satisfaction models helped identify key customer groups for targeted promotions and engagement strategies.</li>
				<li><strong>Improved Retention:</strong> By analyzing customer retention metrics, the study offered guidance on enhancing customer loyalty and reducing churn rates.</li>
			</ul>

		</div>
	</div>
	
	<!-- Modal for Customer Churn Prediction -->
	<div id="ecommerce-project2" class="modal">
		<div class="modal-content">
			<span class="close" onclick="closeModal('ecommerce-project2')">&times;</span>

			<h2>Project Title: Customer Churn Prediction</h2>
			<p><strong>Company:</strong> University Canada West (Research-based)</p>

			<h3>Scope of the Project:</h3>
			<p>To develop a machine learning model that predicts customer churn by analyzing behavioral and transactional data. The goal was to identify key factors contributing to churn and enable proactive retention strategies for businesses. The analysis involved customer demographics, spending habits, service usage, and interaction patterns to enhance customer loyalty and reduce churn rates.</p>

			<h3>Key Responsibilities:</h3>
			<ul>
				<li>Performed Exploratory Data Analysis (EDA) to identify patterns and correlations in customer demographics, spending, and usage behaviors.</li>
				<li>Developed a predictive model using machine learning algorithms to classify customers as likely to churn or remain loyal.</li>
				<li>Implemented data preprocessing techniques such as scaling, encoding, and imputation to handle missing values and prepare the dataset for model training.</li>
				<li>Optimized model performance using techniques like hyperparameter tuning, cross-validation, and feature selection.</li>
				<li>Created visualizations to provide insights into churn factors, including service usage, contract type, payment method, and customer tenure.</li>
				<li>Provided actionable recommendations for targeted customer retention strategies based on model insights.</li>
			</ul>

			<h3>Technologies and Tools Used:</h3>
			<ul>
				<li><strong>Machine Learning Algorithms:</strong> Logistic Regression, Random Forest, Gradient Boosting, and Support Vector Machine (SVM).</li>
				<li><strong>Data Science Tools:</strong> Python, scikit-learn, pandas, NumPy for data manipulation and model building.</li>
				<li><strong>Data Preprocessing:</strong> StandardScaler, OneHotEncoder, and LabelEncoder for handling numerical and categorical variables.</li>
				<li><strong>Evaluation Metrics:</strong> Accuracy, Precision, Recall, F1 Score, and ROC-AUC for model evaluation.</li>
				<li><strong>Visualization:</strong> Matplotlib, Seaborn for churn factor analysis and model performance visualizations.</li>
			</ul>

			<h3>Timeline:</h3>
			<p>This project was conducted as part of the academic curriculum at University Canada West, completed within a semester, from January 2024 to March 2024.</p>

			<h3>Solution Design and Implementation:</h3>
			<ul>
				<li><strong>Data Analysis and Feature Engineering:</strong> Conducted EDA to understand the distribution of key variables such as customer age, service usage, contract type, and monthly charges. Engineered new features like customer tenure categories and interaction frequency to enhance the predictive power of the model.</li>
				<li><strong>Model Development and Tuning:</strong> Split the data into training and testing sets to evaluate model performance. Implemented various classification algorithms, including Logistic Regression, Random Forest, and Gradient Boosting, to predict customer churn. Applied hyperparameter tuning and cross-validation to optimize model performance and prevent overfitting.</li>
				<li><strong>Churn Analysis and Prediction:</strong> Identified the top predictors of churn, such as contract type, payment method, and customer support interactions. Provided a confusion matrix, precision-recall curve, and ROC-AUC score to evaluate the model's effectiveness in identifying high-risk customers.</li>
				<li><strong>Insights and Recommendations:</strong> Generated insights into customer segments with a high likelihood of churn and recommended targeted retention strategies. Suggested interventions like personalized offers, loyalty programs, and proactive customer support to reduce churn rates.</li>
			</ul>

			<h3>Deployment and Impact:</h3>
			<ul>
				<li><strong>Accurate Churn Prediction:</strong> Achieved high predictive accuracy in identifying customers at risk of churning, aiding in timely intervention strategies.</li>
				<li><strong>Informed Retention Strategies:</strong> The analysis provided actionable insights, enabling the business to tailor marketing and retention efforts effectively.</li>
				<li><strong>Business Value:</strong> Helped in reducing customer churn by implementing data-driven retention strategies, improving overall customer lifetime value.</li>
			</ul>

		</div>
	</div>
	
	<!-- Modal for Insurance Premium Prediction -->
	<div id="ecommerce-project3" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('ecommerce-project3')">&times;</span>

				<h2>Project Title: Insurance Premium Prediction</h2>
				<p><strong>Company:</strong> University Canada West (Research-based)</p>

				<h3>Scope of the Project:</h3>
				<p>To develop a predictive model for estimating insurance premiums based on customer demographics, health metrics, and lifestyle factors. The project aimed to enhance risk assessment accuracy, personalize insurance offerings, and optimize pricing strategies for insurance companies. This involved analyzing historical data to identify key factors influencing premium costs and building a model to forecast premiums for new customers.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li>Conducted Exploratory Data Analysis (EDA) to explore customer demographics, health metrics, and insurance-related features.</li>
					<li>Developed a predictive model using regression techniques to estimate insurance premiums based on customer attributes such as age, BMI, smoking status, and region.</li>
					<li>Implemented feature engineering to create new variables, improving model accuracy and interpretability.</li>
					<li>Applied data preprocessing techniques like scaling and encoding to prepare the dataset for model training.</li>
					<li>Performed model evaluation using metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared to assess prediction accuracy.</li>
					<li>Visualized key insights and relationships between features and insurance premiums to support data-driven decision-making.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Machine Learning Algorithms:</strong> Linear Regression, Random Forest Regressor, Gradient Boosting, and Support Vector Regressor (SVR).</li>
					<li><strong>Data Science Tools:</strong> Python, scikit-learn, pandas, NumPy for data analysis, model building, and evaluation.</li>
					<li><strong>Data Preprocessing:</strong> StandardScaler for feature scaling, OneHotEncoder for categorical variables, and handling missing data.</li>
					<li><strong>Evaluation Metrics:</strong> MAE, MSE, R-squared to measure model performance.</li>
					<li><strong>Visualization:</strong> Matplotlib, Seaborn for plotting feature relationships and model performance.</li>
				</ul>

				<h3>Timeline:</h3>
				<p>The project was executed as part of the academic curriculum at University Canada West, spanning a semester from January 2024 to March 2024.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Data Analysis and Feature Engineering:</strong> Performed EDA to understand the distribution and relationships of key variables like age, BMI, number of dependents, and smoking status. Engineered features such as age groups, risk categories, and interaction terms to enhance model performance.</li>
					<li><strong>Model Development and Tuning:</strong> Implemented regression models including Linear Regression, Random Forest Regressor, and Gradient Boosting to predict insurance premiums. Fine-tuned hyperparameters using techniques like GridSearchCV and cross-validation to optimize model accuracy.</li>
					<li><strong>Prediction and Analysis:</strong> Developed a predictive model to forecast insurance premiums, considering customer demographics and lifestyle factors. Evaluated model performance using MAE, MSE, and R-squared to ensure reliable premium estimates.</li>
					<li><strong>Insights and Recommendations:</strong> Identified key factors influencing insurance premiums, such as age, BMI, and smoking status. Provided recommendations for personalized insurance offerings and risk-based premium adjustments based on customer profiles.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Accurate Premium Estimation:</strong> The model provided accurate insurance premium estimates, aiding in risk assessment and pricing strategy optimization.</li>
					<li><strong>Personalized Offerings:</strong> Enabled the customization of insurance products and pricing based on individual risk factors, improving customer satisfaction.</li>
					<li><strong>Business Value:</strong> Assisted insurance companies in enhancing underwriting processes, optimizing pricing, and managing risk effectively.</li>
				</ul>

			</div>
		</div>
		
	   <!-- Modal for Bank Churn Prediction -->
		<div id="finance-project1" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('finance-project1')">&times;</span>

				<h2>Project Title: Bank Churn Prediction</h2>
				<p><strong>Company:</strong> University Canada West (Research-based)</p>

				<h3>Scope of the Project:</h3>
				<p>The project aimed to develop a predictive model to identify customers likely to churn from a banking institution. By analyzing customer demographics, transaction behavior, account details, and service usage, the goal was to provide insights into factors contributing to customer churn and enable the bank to implement targeted retention strategies to minimize attrition rates.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li>Conducted Exploratory Data Analysis (EDA) to explore patterns in customer behavior, demographics, and banking activity.</li>
					<li>Built a predictive model using machine learning algorithms to classify customers based on their likelihood to churn.</li>
					<li>Preprocessed data, including handling missing values, scaling numerical features, and encoding categorical variables for model training.</li>
					<li>Implemented feature engineering to enhance model performance by creating new features, such as transaction frequency, average account balance, and service usage metrics.</li>
					<li>Evaluated the model using various metrics like Accuracy, Precision, Recall, F1 Score, and ROC-AUC to ensure reliable churn predictions.</li>
					<li>Visualized churn drivers and provided actionable insights to support targeted retention strategies.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Machine Learning Algorithms:</strong> Logistic Regression, Random Forest, Gradient Boosting, XGBoost, Support Vector Machine (SVM).</li>
					<li><strong>Data Science Tools:</strong> Python, scikit-learn, pandas, NumPy for data analysis, model building, and evaluation.</li>
					<li><strong>Data Preprocessing:</strong> StandardScaler for feature scaling, OneHotEncoder for categorical encoding, and LabelEncoder for target variable transformation.</li>
					<li><strong>Evaluation Metrics:</strong> Accuracy, Precision, Recall, F1 Score, ROC-AUC for model evaluation.</li>
					<li><strong>Visualization:</strong> Matplotlib, Seaborn for data visualization and model performance analysis.</li>
				</ul>

				<h3>Timeline:</h3>
				<p>The project was executed within a semester as part of the Machine Learning curriculum at University Canada West, completed from January 2024 to March 2024.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Data Analysis and Feature Engineering:</strong> Performed EDA to understand customer distribution across demographics, account balances, transaction patterns, and service usage. Engineered features such as customer tenure, transaction frequency, and service usage intensity to improve the model's predictive power.</li>
					<li><strong>Model Development and Tuning:</strong> Implemented various classification models like Logistic Regression, Random Forest, and Gradient Boosting to predict customer churn. Applied GridSearchCV and cross-validation techniques for hyperparameter tuning to achieve optimal model performance.</li>
					<li><strong>Prediction and Analysis:</strong> Developed a robust churn prediction model to identify high-risk customers. Evaluated the model using metrics like Precision, Recall, and ROC-AUC, ensuring a balanced trade-off between false positives and negatives.</li>
					<li><strong>Insights and Recommendations:</strong> Identified key factors influencing churn, including low account balance, reduced transaction frequency, and limited service usage. Provided actionable recommendations for retention strategies, such as personalized offers, customer engagement programs, and enhanced customer service for high-risk segments.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Accurate Churn Identification:</strong> The model provided high accuracy in predicting bank customers likely to churn, facilitating timely interventions.</li>
					<li><strong>Improved Retention:</strong> Enabled the bank to proactively engage with at-risk customers through targeted marketing and personalized retention strategies.</li>
					<li><strong>Business Value:</strong> Helped reduce customer churn rates, improving overall customer lifetime value and enhancing profitability for the bank.</li>
				</ul>

			</div>
		</div>
		
		
		<!-- Modal for City Businesses Analysis -->
		<div id="finance-project2" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('finance-project2')">&times;</span>

				<h2>Project Title: City Businesses Analysis</h2>
				<p><strong>Company:</strong> Tectonic Technologies</p>

				<h3>Scope of the Project:</h3>
				<p>To analyze the spatial relationship between various businesses and gas stations within a 1 km range in urban areas, with a primary focus on Atlanta, Georgia. The project aimed to identify which types of businesses rely heavily on proximity to gas stations and how the presence of gas stations impacts business patterns. The insights from this analysis assist in strategic business planning and urban development.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li>Conducted an analysis of 2,634 businesses in Atlanta, exploring the spatial relationships between businesses and gas stations within a 1 km radius.</li>
					<li>Identified key business types that heavily rely on proximity to gas stations, calculating the probability of their occurrence near gas stations.</li>
					<li>Performed data extraction and filtering using data sources from the Outscraper website and Google Maps.</li>
					<li>Analyzed the average business count around gas stations and how business density varies with distance.</li>
					<li>Generated reports detailing combinations of businesses around gas stations to evaluate the probability of gas station occurrence when two or more business types are within 1 km.</li>
					<li>Developed visualizations and statistical analyses to highlight the patterns and dependencies between gas stations and surrounding businesses.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Data Analysis Tools:</strong> Python for data processing, filtering, and analysis.</li>
					<li><strong>Data Visualization:</strong> Matplotlib, Seaborn for visualizing spatial relationships and business distribution.</li>
					<li><strong>Data Sources:</strong> Outscraper and Google Maps for gathering location-based data on businesses and gas stations.</li>
					<li><strong>Data Handling:</strong> Excel for initial data extraction, CSV files for filtered data storage.</li>
					<li><strong>Statistical Analysis:</strong> Probability calculation and distance-based business occurrence analysis.</li>
				</ul>

				<h3>Timeline:</h3>
				<p>The project was executed over a course of several weeks in Q4 2022, from initial data gathering to final analysis in December 2022.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Data Collection and Filtering:</strong> Collected raw data for Atlanta city businesses from the Outscraper website, utilizing Google Maps as the primary data source. Filtered and processed the data, resulting in a dataset containing 2,634 businesses, focusing primarily on gas stations and their surrounding businesses.</li>
					<li><strong>Business Dependency Analysis:</strong> Analyzed which business types have the highest dependency on gas stations, such as investment companies, electric vehicle charging stations, and photography studios, with probabilities ranging from 0.90 to 0.97. Generated reports highlighting top businesses relying on gas stations and provided graphical representations for each business type.</li>
					<li><strong>Spatial Analysis:</strong> Assessed the spatial distribution of gas stations around businesses, calculating the average number of businesses within a 1 km range of gas stations. Conducted combination analysis of businesses around gas stations to evaluate the likelihood of gas station presence when two businesses are within 1 km, analyzing 100,000 combinations out of 407,253 possible combinations.</li>
					<li><strong>Visualization and Reporting:</strong> Visualized the average business count around gas stations and the direct proportionality of business density with distance from gas stations. Compiled detailed reports and code files for analysis, providing insights into business patterns in relation to gas station locations.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Business Insights:</strong> Provided critical insights into how various businesses cluster around gas stations, assisting in strategic planning for urban development and business expansion.</li>
					<li><strong>Strategic Decision Making:</strong> Enabled stakeholders to understand business dependencies on gas stations, supporting decisions on new business locations and city infrastructure planning.</li>
					<li><strong>Comprehensive Reports:</strong> Delivered comprehensive analysis reports and visualizations, serving as a valuable resource for city planners and business investors.</li>
				</ul>

			</div>
		</div>

		<!-- Modal for Auto Parts Business Analysis -->
		<div id="finance-project3" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('finance-project3')">&times;</span>

				<h2>Project Title: Auto Parts Business Analysis</h2>
				<p><strong>Company:</strong> VAP Auto Parts</p>

				<h3>Scope of the Project:</h3>
				<p>To analyze and gather valuable insights from businesses in Surrey related to the auto parts industry. The project involved web scraping and data extraction from various business websites to collect information that could be leveraged for targeted promotions and advertisements. The goal was to use this data to enhance marketing strategies for VAP Auto Parts, ensuring effective outreach and engagement with potential customers.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li>Conducted web scraping to collect data on businesses in Surrey that are relevant to the auto parts industry.</li>
					<li>Extracted valuable information from business websites, including service offerings, customer reviews, business hours, and contact details.</li>
					<li>Analyzed the collected data to identify key trends and patterns in the auto parts market in Surrey.</li>
					<li>Utilized insights from the analysis to develop targeted promotional strategies and advertising campaigns for VAP Auto Parts.</li>
					<li>Created a structured database of the gathered information for future marketing efforts and customer outreach programs.</li>
					<li>Provided actionable recommendations to the marketing team for enhancing VAP Auto Parts' advertising effectiveness based on competitor analysis and market insights.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Web Scraping Tools:</strong> BeautifulSoup, Selenium for automated data extraction from business websites.</li>
					<li><strong>Data Analysis Tools:</strong> Python, pandas, and NumPy for processing and analyzing the scraped data.</li>
					<li><strong>Data Storage:</strong> CSV and SQL databases for storing and managing the extracted information.</li>
					<li><strong>Visualization Tools:</strong> Matplotlib, Seaborn for visualizing market trends and insights.</li>
					<li><strong>Data Cleaning and Preprocessing:</strong> Regular expressions and custom parsers to clean and normalize data for analysis.</li>
				</ul>

				<h3>Timeline:</h3>
				<p>The project was completed in Q3 2023, with phases spanning data scraping, analysis, and report generation over a period of three months.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Data Collection:</strong> Scraped data from various business websites in Surrey, focusing on those related to the auto parts industry. Extracted key information such as product offerings, customer reviews, business hours, and contact information.</li>
					<li><strong>Data Cleaning and Analysis:</strong> Cleaned and processed the scraped data to remove duplicates, normalize formats, and extract relevant insights. Analyzed data to identify market trends, customer preferences, and service gaps in the auto parts industry within Surrey.</li>
					<li><strong>Market Analysis and Strategy Development:</strong> Conducted a competitor analysis to understand the strengths and weaknesses of other auto parts businesses in Surrey. Used insights to develop targeted advertising campaigns aimed at promoting VAP Auto Parts’ offerings.</li>
					<li><strong>Reporting and Recommendations:</strong> Compiled findings into actionable insights for the marketing team. Provided recommendations on promotional strategies, including targeted advertisements, special offers, and customer engagement initiatives.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Enhanced Market Understanding:</strong> Provided a comprehensive understanding of the auto parts market in Surrey, identifying key areas for strategic advertising.</li>
					<li><strong>Targeted Marketing Campaigns:</strong> Enabled VAP Auto Parts to launch more effective promotional campaigns based on detailed market and competitor insights.</li>
					<li><strong>Increased Outreach:</strong> Improved customer outreach and engagement by tailoring advertising strategies to the preferences and needs of the target market.</li>
				</ul>

			</div>
		</div>

		<!-- Modal for The VAP_App - Auto Parts Management System -->
		<div id="finance-project4" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('finance-project4')">&times;</span>

				<h2>Project Title: Auto Parts Management System</h2>
				<p><strong>Company:</strong> VAP Auto Parts</p>

				<h3>Scope of the Project:</h3>
				<p>The VAP_App is a Django-based web application designed to manage and visualize auto parts, sales, and car intake data across multiple locations. The primary objective was to create an interactive platform that provides detailed insights through dashboards, charts, and reports. The application enables store managers and business analysts to efficiently analyze sales trends, inventory status, and car intake, improving decision-making and operational efficiency.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li><strong>Development and Design:</strong> Built a responsive web application using Django framework, integrating HTML, CSS, Bootstrap, and JavaScript for frontend development.</li>
					<li><strong>Data Management:</strong> Implemented CSV-based data storage and handling for managing sales, parts, and car intake data across various locations.</li>
					<li><strong>Interactive Dashboards:</strong> Designed multiple sections including Dashboard, Parts, Cars, and Reports to display real-time data through interactive charts and tables.</li>
					<li><strong>Feature Implementation:</strong> Developed features like monthly and daily sales tracking, parts inventory visualization, car intake analysis, and report generation with date filters and store selection.</li>
					<li><strong>Data Visualization:</strong> Integrated Plotly for creating dynamic charts and graphs to visually represent sales data, inventory levels, and car intake statistics.</li>
					<li><strong>User Interface:</strong> Enhanced user experience with interactive elements like dropdowns, date pickers, and buttons for seamless navigation and report generation.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Frontend:</strong> HTML, CSS, Bootstrap, JavaScript for responsive design and user interaction.</li>
					<li><strong>Backend:</strong> Django framework for backend development and server-side logic.</li>
					<li><strong>Data Handling:</strong> MS Database for data storage, manipulation, and analysis.</li>
					<li><strong>Cache:</strong> Redis for storing cache data.</li>
					<li><strong>Data Visualization:</strong> Plotly for creating interactive charts and graphs.</li>
					<li><strong>Styling and Design:</strong> Bootstrap for styling and ensuring a consistent look and feel across the application.</li>
				</ul>

				<h3>Timeline:</h3>
				<p>The project was developed over several months in 2023, with key phases including design, development, testing, and deployment.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Dashboard:</strong> Implemented features to display monthly and daily sales data across various locations such as Vancouver, Surrey, Port Moody, Burnaby, and Richmond. Designed interactive charts to show sales trends, average sales, and category-wise sales. Created a recent sales table to provide a quick overview of recent transactions with details like date, invoice, customer, amount, and payment type.</li>
					<li><strong>Parts Management:</strong> Developed modules to display part sales data, including monthly, weekly, and daily figures. Integrated location-specific sales data, allowing users to filter and view sales by store. Incorporated charts to visualize parts sales data and tables to list recent parts transactions.</li>
					<li><strong>Car Intake:</strong> Created sections to track car intake data on a monthly, weekly, and daily basis across different locations. Implemented an interactive dashboard to allow detailed analysis of car intake patterns.</li>
					<li><strong>Report Generation:</strong> Added functionality to generate various reports such as Transfer, Supplier, Cashier, Category, Sales, Inventory, and Clerk Performance. Implemented dropdowns for store selection and date pickers for generating time-specific reports. Enabled the analysis of selected reports, providing detailed results for informed decision-making.</li>
					<li><strong>User Interaction and Navigation:</strong> Designed a user-friendly interface with consistent navigation to switch seamlessly between different sections of the application. Integrated interactive elements like dropdowns and date pickers to enhance user experience and facilitate in-depth data analysis.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Enhanced Data Management:</strong> Centralized data management for sales, parts inventory, and car intake, providing comprehensive insights into business operations.</li>
					<li><strong>Informed Decision-Making:</strong> The application enabled store managers to make data-driven decisions through visual dashboards and detailed reports.</li>
					<li><strong>Operational Efficiency:</strong> Streamlined the analysis process for sales, inventory, and car intake, saving time and improving the accuracy of business insights.</li>
					<li><strong>Improved User Experience:</strong> The intuitive interface and interactive features made it easier for users to navigate and analyze data, enhancing overall usability.</li>
				</ul>

				<h3>Usage:</h3>
				<ul>
					<li><strong>Interactive Interface:</strong> Users can interact with the application using dropdowns, date pickers, and buttons to filter and generate reports.</li>
					<li><strong>Data Analysis:</strong> The app provides tools for analyzing sales, parts, and car data visually through graphs and detailed tables.</li>
					<li><strong>Navigation:</strong> A consistent navigation bar allows users to switch between sections like Dashboard, Parts, Cars, and Reports seamlessly.</li>
				</ul>

			</div>
		</div>

		
		<!-- Modal for The Technician App - Automotive Maintenance Management System -->
		<div id="soft-project1" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('soft-project1')">&times;</span>

				<h2>Project Title: The Technician App - Automotive Maintenance Management System</h2>
				<p><strong>Company:</strong> VAP Auto Parts</p>

				<h3>Scope of the Project:</h3>
				<p>To streamline automotive maintenance workflows for technicians by providing a comprehensive Django-based system. The application includes features such as user authentication, OCR for extracting data from insurance papers, work order management, and real-time notifications. It aims to enhance efficiency in managing work orders, improve data accuracy, and facilitate effective communication between technicians, staff, and customers.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li>Developed a secure user authentication system using JWT to manage user access and sessions.</li>
					<li>Integrated Optical Character Recognition (OCR) to automatically extract information from insurance papers, reducing manual data entry and errors.</li>
					<li>Implemented work order management functionalities to allow technicians to create, update, and track work orders with detailed customer and vehicle information.</li>
					<li>Designed a notification system to send real-time updates to technicians and customers via push notifications, SMS, and email.</li>
					<li>Built data management features to handle inventory, branches, and work order data securely and efficiently.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Backend:</strong> Django, JWT (JSON Web Tokens) for user authentication.</li>
					<li><strong>OCR and Image Processing:</strong> Tesseract OCR, OpenCV for text extraction and image enhancement.</li>
					<li><strong>Communication:</strong> Twilio for SMS notifications, Apple Push Notification service, and custom email sending functionality.</li>
					<li><strong>Data Handling:</strong> Python, SQLite/PostgreSQL for database management.</li>
					<li><strong>Frontend:</strong> HTML, CSS, JavaScript for responsive user interfaces.</li>
				</ul>

				<h3>Timeline:</h3>
				<p>Developed and completed in 2023 over a period of six months.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Authentication and Authorization:</strong> Implemented JWT-based authentication for secure user login and session management. Provided user login and logout endpoints, ensuring secure access to work order data and features.</li>
					<li><strong>OCR for Insurance Papers:</strong> Integrated OCR technology to extract and process data from insurance papers, including owner's name, address, and vehicle details. Implemented image preprocessing techniques like rotation, contrast enhancement, and denoising to improve OCR accuracy.</li>
					<li><strong>Work Order Management:</strong> Developed features for creating and managing work orders, including adding customer and vehicle information. Enabled technicians to update work orders with inspection results, feedback, and status updates.</li>
					<li><strong>Notifications and Communication:</strong> Built a notification system to send real-time alerts to technicians and customers via push notifications, SMS, and email. Enhanced customer communication by providing updates on work order status and estimated completion times.</li>
					<li><strong>Data Filtration and Management:</strong> Designed data management functionalities to filter and process data related to technicians, branches, and inventory. Ensured secure data handling and processing to maintain data integrity and confidentiality.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Streamlined Workflow:</strong> Automated and simplified the lifecycle of automotive service tasks, improving efficiency and reducing manual errors.</li>
					<li><strong>Improved Communication:</strong> Enhanced communication between technicians and customers through timely notifications and updates.</li>
					<li><strong>Accurate Data Management:</strong> Reduced manual data entry and improved accuracy by using OCR for data extraction from insurance papers.</li>
				</ul>

			</div>
		</div>
		
		<!-- Modal for Medical Insurance Cost Prediction -->
		<div id="healthcare-project1" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('healthcare-project1')">&times;</span>

				<h2>Project Title: Medical Insurance Cost Prediction</h2>
				<p><strong>Company:</strong> UCW - Research Project</p>

				<h3>Scope of the Project:</h3>
				<p>The goal of this project was to develop a predictive model to estimate the insurance costs for individuals based on their demographics, health metrics, and lifestyle factors. By analyzing variables such as age, BMI, smoking status, and region, the model helps insurance providers accurately assess risk and personalize insurance premiums. This project aimed to enhance risk assessment accuracy and support fair and data-driven insurance pricing strategies.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li>Conducted Exploratory Data Analysis (EDA) to identify patterns and relationships in the dataset, including variables like age, BMI, smoking status, number of children, and region.</li>
					<li>Developed regression models to predict insurance costs using various algorithms such as Linear Regression, Random Forest, and Gradient Boosting.</li>
					<li>Performed data preprocessing, including handling missing values, scaling numerical features, and encoding categorical variables for effective model training.</li>
					<li>Conducted feature engineering to create new variables and improve model performance.</li>
					<li>Evaluated model performance using metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared to ensure the accuracy of cost predictions.</li>
					<li>Visualized key insights and relationships between features and insurance costs to aid in decision-making.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Machine Learning Algorithms:</strong> Linear Regression, Random Forest Regressor, Gradient Boosting, XGBoost.</li>
					<li><strong>Data Science Tools:</strong> Python, scikit-learn, pandas, NumPy for data analysis, model building, and evaluation.</li>
					<li><strong>Data Preprocessing:</strong> StandardScaler for feature scaling, OneHotEncoder for categorical variables.</li>
					<li><strong>Evaluation Metrics:</strong> MAE, MSE, R-squared to measure model accuracy.</li>
					<li><strong>Visualization:</strong> Matplotlib, Seaborn for visualizing data distributions and model performance.</li>
				</ul>

				<h3>Timeline:</h3>
				<p>The project was executed over a period of three months, encompassing phases from data collection to model deployment.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Data Analysis and Feature Engineering:</strong> Performed EDA to understand the distribution of key variables such as age, BMI, smoking status, and their impact on insurance costs. Engineered features like age groups and BMI categories to enhance the model's predictive power.</li>
					<li><strong>Model Development and Tuning:</strong> Implemented various regression models, including Linear Regression, Random Forest Regressor, and Gradient Boosting, to predict insurance costs. Applied hyperparameter tuning using techniques like GridSearchCV to optimize model performance.</li>
					<li><strong>Prediction and Analysis:</strong> Developed a predictive model that accurately estimates insurance premiums based on individual health and demographic factors. Evaluated the model using MAE, MSE, and R-squared metrics to ensure reliable and accurate predictions.</li>
					<li><strong>Insights and Recommendations:</strong> Identified key factors influencing insurance costs, such as age, BMI, smoking status, and region. Provided recommendations for adjusting premiums based on individual risk factors to support personalized and fair pricing.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Accurate Cost Prediction:</strong> The model achieved high accuracy in estimating insurance costs, aiding insurance providers in fair and risk-adjusted premium pricing.</li>
					<li><strong>Personalized Insurance Offerings:</strong> Enabled more personalized insurance plans by taking into account individual health metrics and lifestyle factors.</li>
					<li><strong>Business Value:</strong> Assisted in optimizing pricing strategies, improving risk assessment, and enhancing customer satisfaction through transparent and fair premium calculations.</li>
				</ul>

			</div>
		</div>
		
	<!-- Modal for Abalone Case Study - Age Prediction and Classification -->
	<div id="healthcare-project2" class="modal">
	<div class="modal-content">
		<span class="close" onclick="closeModal('healthcare-project2')">&times;</span>

		<h2>Project Title: Abalone Case Study - Age Prediction and Classification</h2>
		<p><strong>Company:</strong> University Canada West (Research-based)</p>

		<h3>Scope of the Project:</h3>
		<p>The project aimed to analyze a dataset of abalone, a type of marine gastropod, to understand the relationship between physical attributes and their age. Using machine learning techniques such as classification and clustering, the project explored the physical attributes like shell measurements and weight to predict the age of abalones. This study also involved exploring the data distribution, applying classification algorithms like K-Nearest Neighbors (KNN), and utilizing clustering methods such as K-Means to uncover patterns within the dataset.</p>

		<h3>Key Responsibilities:</h3>
		<ul>
			<li><strong>Data Analysis:</strong> Conducted an exploratory data analysis on a dataset containing 4,177 observations and 9 features, including measurements such as shell diameter, height, and weight.</li>
			<li><strong>Feature Engineering:</strong> Analyzed the distribution of features, handled skewed distributions, and identified outliers in continuous variables.</li>
			<li><strong>Classification:</strong> Employed the K-Nearest Neighbors (KNN) algorithm to classify abalones, focusing on optimizing model performance by tuning parameters like the number of neighbors (k).</li>
			<li><strong>Clustering:</strong> Implemented K-Means clustering and applied PCA (Principal Component Analysis) to reduce dimensionality for visualization and analysis of abalone clusters.</li>
			<li><strong>Model Evaluation:</strong> Assessed the performance of models using metrics like accuracy, precision, recall, confusion matrix, and silhouette score for clustering quality.</li>
			<li><strong>Visualization:</strong> Created graphs and charts to depict data distributions, classification performance, and clustering results.</li>
		</ul>

		<h3>Technologies and Tools Used:</h3>
		<ul>
			<li><strong>Programming and Libraries:</strong> Python, pandas, NumPy, scikit-learn for data processing, model implementation, and evaluation.</li>
			<li><strong>Data Visualization:</strong> Matplotlib, Seaborn for visualizing distributions, model performance, and clustering.</li>
			<li><strong>Machine Learning Algorithms:</strong> K-Nearest Neighbors (KNN) for classification, K-Means for clustering.</li>
			<li><strong>Dimensionality Reduction:</strong> Principal Component Analysis (PCA) to reduce data dimensionality for clustering visualization.</li>
			<li><strong>Evaluation Metrics:</strong> Accuracy, precision, recall, confusion matrix for classification; silhouette score and elbow method for clustering.</li>
		</ul>

		<h3>Timeline:</h3>
		<p>The project was conducted in early 2024 as part of the Machine Learning Tools and Techniques course.</p>

		<h3>Solution Design and Implementation:</h3>
		<ul>
			<li><strong>Dataset Overview:</strong> Analyzed 4,177 observations with attributes like shell length, diameter, height, and various weight measurements. Identified the target variable "Type" (dependent variable) and explored the distribution of independent variables, noting skewness and outliers.</li>
			<li><strong>Classification with KNN:</strong> Split the dataset into training (80%) and testing (20%) sets. Used KNN for classification, starting with 2-Nearest Neighbors (2NN) and a random approach to identify the optimal value of 'k'. Achieved a test accuracy of 51% with 2NN and 53% with random neighbor selection, indicating room for further optimization.</li>
			<li><strong>Clustering with K-Means:</strong> Applied K-Means clustering with an initial 'k' value of 5, followed by PCA to visualize clusters in a 2D space. Used the Elbow Method to determine the optimal number of clusters, identifying 'k=3' as the best fit based on the elbow chart and silhouette score.</li>
			<li><strong>Model Evaluation:</strong> Evaluated classification models using accuracy, precision, and recall metrics. Assessed clustering quality using the silhouette score, finding a score of approximately 0.515 for 'k=3'.</li>
			<li><strong>Visualization and Insights:</strong> Visualized the data distributions, classification confusion matrices, and clustering results to understand the patterns in the dataset. Identified that further feature selection could enhance the performance of the classification models.</li>
		</ul>

		<h3>Deployment and Impact:</h3>
		<ul>
			<li><strong>Understanding Abalone Age:</strong> Provided insights into the physical characteristics that could indicate the age of abalones.</li>
			<li><strong>Model Performance:</strong> Highlighted the strengths and limitations of using KNN and K-Means for classification and clustering in biological datasets.</li>
			<li><strong>Data Analysis Skills:</strong> Demonstrated proficiency in data preprocessing, model implementation, and evaluation, contributing to the broader understanding of abalone biology and sustainable fisheries management.</li>
		</ul>

	</div>
	</div>
	
	<!-- Modal for Driver App - Task Management and Communication System -->
		<div id="soft-project2" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('soft-project2')">&times;</span>

				<h2>Project Title: Driver App - Task Management and Communication System</h2>
				<p><strong>Company:</strong> VAP Auto Parts</p>

				<h3>Scope of the Project:</h3>
				<p>The Driver App is a comprehensive web application designed to streamline task management, communication, and workflow for drivers and administrators. It offers features like task assignment, real-time notifications, messaging, and priority management within an intuitive and user-friendly interface. The app aims to enhance operational efficiency, improve communication, and provide a centralized platform for task management, particularly for businesses in delivery services, logistics, and field services.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li><strong>Task Management:</strong> Implemented task assignment, tracking, and prioritization features to allow administrators to manage and monitor driver workloads in real-time. Included functionalities for setting task priorities, tracking task history, and filtering tasks based on status and dates.</li>
					<li><strong>Real-Time Notifications and Messaging:</strong> Developed a real-time notification system to alert drivers and administrators about task updates, messages, and important alerts. Integrated in-app and desktop notifications for seamless communication, along with direct messaging capabilities for instant interaction.</li>
					<li><strong>User Management:</strong> Designed a role-based user management system to add, edit, and manage drivers and cashiers. Provided options to activate, deactivate, or delete users based on their status, ensuring secure access control.</li>
					<li><strong>Interactive UI and Workflow Enhancements:</strong> Created a dynamic and responsive user interface using JavaScript and jQuery for smooth content updates. Implemented sidebar navigation, modals, and forms for an intuitive user experience.</li>
					<li><strong>Search and Filtering:</strong> Enabled advanced search and data filtering features to allow users to quickly find tasks and messages based on various criteria, enhancing the app's usability.</li>
					<li><strong>Error Handling and Feedback:</strong> Integrated error messages and loading spinners to provide clear feedback to users during operations, ensuring transparency and an informed user experience.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Frontend:</strong> HTML, CSS, JavaScript, jQuery for interactive and dynamic UI components.</li>
					<li><strong>Backend:</strong> Django framework for building the web application's server-side logic.</li>
					<li><strong>Real-Time Communication:</strong> WebSockets for real-time messaging and notifications.</li>
					<li><strong>Database:</strong> PostgreSQL/SQLite for data storage and management of tasks, user information, and communication logs.</li>
					<li><strong>Data Handling:</strong> REST APIs for data fetching, task updates, and managing user information.</li>
				</ul>

				<h3>Timeline:</h3>
				<p>The project was developed over six months, including design, development, testing, and deployment phases in 2023.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Task Management:</strong> Developed task assignment and tracking features, allowing administrators to assign tasks to drivers with real-time status updates. Implemented task prioritization with drag-and-drop functionality to adjust task order, ensuring critical tasks are handled promptly. Created a task history feature with filtering options to track performance and revisit past activities.</li>
					<li><strong>Real-Time Notifications and Messaging:</strong> Integrated instant alerts for task updates, messages, and critical information, utilizing both in-app and desktop notifications. Enabled direct messaging between drivers and administrators to facilitate quick communication and reduce miscommunication.</li>
					<li><strong>User Management:</strong> Built a user management system with role-based access, allowing for the addition, editing, and status management of drivers and cashiers. Ensured secure access control to maintain the integrity and confidentiality of sensitive information.</li>
					<li><strong>Interactive UI and Workflow Enhancements:</strong> Designed an interactive UI with dynamic content loading to provide a smooth user experience without full-page reloads. Implemented sidebar navigation, modals, and forms for streamlined task management and easy navigation between sections.</li>
					<li><strong>Search and Filtering:</strong> Added advanced search and filtering capabilities to allow users to quickly locate tasks based on criteria like task ID, assigned driver, and status. Enhanced data filtering options to provide a focused view of relevant information.</li>
					<li><strong>Error Handling and Feedback:</strong> Integrated error messages and loading spinners to inform users about the system's status during operations like data fetching and task updates.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Improved Operational Efficiency:</strong> Centralized task management reduced the time and effort needed to manage drivers' workflows, improving overall efficiency.</li>
					<li><strong>Enhanced Communication:</strong> Real-time notifications and messaging facilitated quick and direct communication, ensuring that tasks were completed accurately and on time.</li>
					<li><strong>Better Task Prioritization:</strong> The task prioritization feature helped ensure that critical tasks were handled first, optimizing resource use and improving customer satisfaction.</li>
					<li><strong>User-Friendly Interface:</strong> An intuitive UI with interactive elements made the application easy to use, reducing training time and minimizing user errors.</li>
					<li><strong>Data-Driven Decision Making:</strong> The app's historical data access and performance metrics enabled administrators to make informed decisions and enhance operational strategies.</li>
				</ul>

			</div>
		</div>
		
		<!-- Modal for Project Symmetry - Semantic Comparison Tool -->
		<div id="healthcare-project3" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('healthcare-project3')">&times;</span>

				<h2>Project Title: Project Symmetry - Semantic Comparison Tool</h2>
				<p><strong>Company:</strong> Grey-Box</p>

				<h3>Scope of the Project:</h3>
				<p>Project Symmetry aims to bridge the information gap in Wikipedia articles across different languages, particularly those that are underrepresented online. By using semantic comparison algorithms, the tool identifies discrepancies and missing information between articles in different languages, providing a means to enhance the quality and completeness of Wikipedia content globally. The tool uses Natural Language Processing (NLP) to compare and translate articles, highlighting differences and assisting users in improving translations.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li><strong>Problem Analysis:</strong> Addressed the lack of equal access to information in Wikipedia articles across different languages, especially for underrepresented regions.</li>
					<li><strong>NLP Integration:</strong> Incorporated NLP models like BLEU Score and Sentence BERT to perform semantic comparison between Wikipedia articles in different languages.</li>
					<li><strong>Web Scraping and Translation:</strong> Developed a web scraper to extract and translate Wikipedia articles, supporting multiple languages and enabling users to compare content accurately.</li>
					<li><strong>GUI and User Experience:</strong> Enhanced the Graphical User Interface (GUI) using PySimpleGUI to provide a user-friendly experience, allowing users to input article links, select languages, and view comparison results.</li>
					<li><strong>User Guide and Error Handling:</strong> Created a comprehensive user guide to facilitate tool usage and implemented warning/error messages to prevent crashes and optimize performance.</li>
					<li><strong>System Optimization:</strong> Improved the system's performance to handle large articles efficiently, ensuring accurate comparisons within a maximum timeframe of two minutes.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Programming Languages and Libraries:</strong> Python, PySimpleGUI, tkinter, BeautifulSoup for web scraping, DeepL API for translations.</li>
					<li><strong>NLP Models:</strong> BLEU Score, Sentence BERT for semantic text comparison.</li>
					<li><strong>Data Handling:</strong> NumPy, scikit-learn, nltk, sentence_transformers for processing and analyzing text.</li>
					<li><strong>Version Control and Development:</strong> Git, GitHub for collaborative development and version control.</li>
				</ul>

				<h3>Timeline:</h3>
				<p>The project was undertaken as a capstone project at the University at Albany, SUNY, with multiple phases of development and refinement.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Problem Analysis:</strong> Identified the disconnect in information across Wikipedia articles in different languages due to unequal internet access globally. Explored existing solutions like browser translation tools, manual translations, and content translation tools on Wikipedia, highlighting their limitations.</li>
					<li><strong>NLP Modeling and Comparison:</strong> Integrated NLP models such as BLEU Score and Sentence BERT to compare articles semantically and provide similarity percentages. Offered users a choice between two comparison algorithms and displayed differences in a user-friendly manner.</li>
					<li><strong>Web Scraping and Translation:</strong> Developed a web scraper to retrieve article content and available languages directly from Wikipedia. Enhanced translation capabilities with improved language selection and used DeepL API to facilitate accurate translations.</li>
					<li><strong>User Interface and Experience:</strong> Created an intuitive GUI to enable users to input Wikipedia article links, select languages, and view comparison results. Implemented an expanded view feature to explicitly show matching sentences and an error handling system to enhance usability.</li>
					<li><strong>Performance Optimization:</strong> Optimized the tool to handle large Wikipedia articles efficiently, ensuring accurate comparisons within a two-minute time frame. Provided download functionality for both source and target articles, allowing users to edit and improve translations.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Enhanced Information Access:</strong> Enabled users to compare and improve Wikipedia articles across different languages, particularly benefiting underrepresented regions.</li>
					<li><strong>Semantic Analysis:</strong> Offered a novel approach to semantic comparison, identifying missing content and discrepancies to improve the quality of Wikipedia articles.</li>
					<li><strong>User-Friendly Tool:</strong> Delivered a user-friendly tool with a comprehensive guide, making it accessible for both regular users and contributors to enhance Wikipedia content.</li>
					<li><strong>Future Scope:</strong> The project sets the foundation for expanding the tool to compare custom content beyond Wikipedia, further aiding in the global dissemination of information.</li>
				</ul>

			</div>
		</div>
		
		<!-- Modal for Barcode App - Inventory Management and Barcode Generation -->
		<div id="soft-project3" class="modal">
			<div class="modal-content">
				<span class="close" onclick="closeModal('soft-project3')">&times;</span>

				<h2>Project Title: Barcode App - Inventory Management and Barcode Generation</h2>
				<p><strong>Company:</strong> VAP Auto Parts</p>

				<h3>Scope of the Project:</h3>
				<p>The Barcode App is a Django-based web application designed to generate, manage, and print barcodes for inventory items. It provides an all-in-one solution for businesses that require efficient inventory tracking and labeling, integrating features such as barcode generation, inventory filtering, and printing capabilities. This app is particularly useful for retail businesses, warehouses, and operations that rely on precise inventory control.</p>

				<h3>Key Responsibilities:</h3>
				<ul>
					<li><strong>Barcode Generation:</strong> Developed functionalities to dynamically generate barcodes for inventory items based on part numbers and categories, supporting various barcode formats such as Code128. Implemented the inclusion of product names on barcode images to improve inventory identification.</li>
					<li><strong>Inventory Management:</strong> Integrated the application with an inventory database, allowing users to search, filter, and select items for barcode generation. Included options for stock and location-based filtering to streamline barcode management.</li>
					<li><strong>Printing Capabilities:</strong> Implemented direct integration with local printers for immediate barcode label printing. Included features for adjusting label margins and ensuring barcode quality.</li>
					<li><strong>Interactive User Interface:</strong> Designed an interactive UI using HTML, JavaScript, and custom scripts to facilitate user input, including part selection, category filtering, and batch barcode generation.</li>
					<li><strong>Data Processing and Validation:</strong> Built backend processes for validating user inputs, querying the inventory database, and handling errors to provide immediate feedback on operations.</li>
					<li><strong>Caching and Performance Optimization:</strong> Implemented caching to store frequently accessed data, optimizing the application's performance and improving the user experience.</li>
				</ul>

				<h3>Technologies and Tools Used:</h3>
				<ul>
					<li><strong>Backend:</strong> Django for web application logic and data handling.</li>
					<li><strong>Frontend:</strong> HTML, CSS, JavaScript, jQuery for creating an interactive user interface.</li>
					<li><strong>Barcode Generation:</strong> Python libraries for generating barcodes in various formats, ensuring compatibility with standard label printers.</li>
					<li><strong>Database:</strong> SQLite/PostgreSQL for managing inventory data and integrating with the barcode generation system.</li>
					<li><strong>Printing Integration:</strong> Printer configuration support for devices like 'ZDesigner ZD410-203dpi ZPL' to ensure compatibility with standard barcode label printers.</li>
				</ul>

				<h3>Timeline:</h3>
				<p>The project was developed over a period of four months in 2023, including phases for requirements gathering, development, testing, and deployment.</p>

				<h3>Solution Design and Implementation:</h3>
				<ul>
					<li><strong>Barcode Generation and Printing:</strong> Developed a barcode generation system using the barCodes module, allowing users to create barcodes dynamically for products based on inventory data. Integrated print functionality for direct label printing, including support for margin adjustments and image processing to ensure high-quality barcode labels.</li>
					<li><strong>Inventory Management:</strong> Connected the app with the inventory database to facilitate real-time barcode generation based on current stock levels. Provided filtering options to narrow down inventory by stock status (e.g., in-stock, min-stock) and location (e.g., Surrey, Port Moody, Vancouver), ensuring precise management of barcode generation.</li>
					<li><strong>Interactive User Interface:</strong> Designed an intuitive UI using HTML, custom JavaScript, and jQuery for dynamic content loading, including dropdowns, pagination, and batch selection features. Enabled users to add multiple parts for barcode generation, facilitating batch processing and improving workflow efficiency.</li>
					<li><strong>Data Processing and Validation:</strong> Implemented backend processing to validate user input, query the inventory database, and prepare data for barcode generation. Included error handling mechanisms to provide feedback on unsuccessful operations, such as missing inventory items or invalid selections.</li>
					<li><strong>Printing Configuration:</strong> Integrated with specific printers for barcode label printing, including options for customization such as adjusting margins and text placement. Supported standard label printers to ensure compatibility with various business requirements.</li>
				</ul>

				<h3>Deployment and Impact:</h3>
				<ul>
					<li><strong>Streamlined Inventory Management:</strong> Automated barcode creation and printing, reducing manual efforts and minimizing errors in inventory labeling.</li>
					<li><strong>Enhanced Operational Efficiency:</strong> Enabled batch processing for barcode generation and printing, saving time for businesses with large inventory volumes.</li>
					<li><strong>User-Friendly Interface:</strong> Provided an interactive and intuitive UI, making it accessible to users with varying technical skills, reducing the learning curve.</li>
					<li><strong>Customizable and Scalable:</strong> Offered customizable barcode label options and robust data handling capabilities, making the app scalable for businesses of different sizes and inventory needs.</li>
				</ul>

			</div>
		</div>
	
    <!-- Footer Section -->
    <footer>
        <p>&copy; 2024 Ahmad Ahsan Saleem. All Rights Reserved.</p>
    </footer>
	
	<script src="js/script.js"></script>

</body>
</html>
